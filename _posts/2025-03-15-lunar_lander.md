---
title:  "Stick the Landing: Continuous-Action RL Meets the Moon"
mathjax: true
layout: post
categories: 
  = github
  - website
---

<div style="text-align: center;">
  <img src="http://kodendaal.github.io/assets/lunar_lander.png" alt="Lunar Lander" style="width: 800px; height: auto;">
</div>

*A chronicle of how one small step for hyper-parameters became a giant leap for continuous-action control.*

---

## What‚Äôs harder than parallel parking in traffic?  
Landing a moon-lander with **throttle values that slide, not click**. In the Lunar Lander-Continuous environment you steer two thrusters‚Äîmain and side‚Äîeach able to fire anywhere between ‚àí1 and +1. Infinite actions, unforgiving physics, and exactly **zero parking sensors**.   

### Why classic Q-learning sulks here  
Deep Q-Networks shine when you can simply say ‚Äúaction #3,‚Äù but asking them to sweep an *infinite* action set would melt your GPU. We need a policy that *outputs* real-valued thrust directly. Enter **policy-gradient actor‚Äìcritic land**!   

---

## Meet today‚Äôs hero: Deep Deterministic Policy Gradient (DDPG)  
Think of DDPG as two neural-network buddies:  

| Role | Job | Fun twist |
|------|-----|-----------|
| **Actor** | Decides: ‚ÄúFire main at 0.42, side at ‚Äì0.08.‚Äù | Gets exploratory jitters from Gaussian noise. |
| **Critic** | Judges that move‚Äôs long-term value. | Uses a slowly moving *target* twin to stay chill. |

Both share an experience-replay scrapbook so they don‚Äôt overreact to any single bumpy touchdown.   

**Exploration hack:** Instead of the fancy Ornstein-Uhlenbeck process, we simply sprinkled zero-mean Gaussian noise on every thrust command‚Äîless tuning, same lunar chaos.   

---

## Rocket science, but reproducible  
* Training: 1 000 episodes √ó 1 000 steps max, ~1 h on a humble 4-core CPU.  
* Networks: 2 hidden layers, 256 neurons each, ReLU everywhere except a *tanh* at the actor‚Äôs output to keep actions in [‚àí1, 1].  
* Lifesavers: gradient clipping (¬±5) after a few wild loss spikes.   

---

## First light: the baseline launch log  
Early episodes looked like *SpaceX prototypes*: spectacular hops, frequent explosions, reward graph doing the samba. But by episode ~800 the curve flirted with the ‚Äúmission-success‚Äù line of **200 points average**. Default settings finally landed at **‚âà 249 ¬± 57** on the last run‚Äîmission technically accomplished!   

---

## Turbo-tuning with Optuna  
Four knobs went into the Bayesian blender:

1. **Discount Œ≥** (0.90 ‚Äì 0.99)  
2. **Soft target œÑ** (0.0001 ‚Äì 0.01)  
3. **Batch size** {32, 64, 128}  
4. **Update frequency** {1 ‚Äì 5 actor/critic steps per env step}   

### Surprise winners  
* **Œ≥ ‚âà 0.985**‚Äîbig-picture thinking pays off, but Œ≥ > 0.99 made the agent dreamy and slow.  
* **œÑ ‚âà 0.0045**‚Äîfast enough to learn, slow enough to stay zen.  
* **Batch 128**‚Äîmore samples, smoother gradients, no memory drama.  
* **Updates every 2-4 steps**‚Äîtoo eager and you chase noise; too lazy and you stagnate.   

Best trial (call sign **T-4**) cruised to **264 ¬± 23** with steadier landings and smaller variances. Not bad for 20 tries!   

*(Fun fact: a hand-coded PID heuristic still scored ~290, proving that humans with equations can out-fly RL‚Äî**for now**.)*  

---

## What nearly cratered us  

| Oops moment | Quick fix |
|-------------|-----------|
| Critic loss skyrocketed ‚Üí actor followed bad advice | Gradient clipping ¬±5 |
| Vanishing actor gradients (tanh saturation) | Considering Leaky ReLU next round |
| Critic over-estimates Q (DDPG classic flaw) | TD3 beckons on the roadmap |   

---

## Lessons from low lunar orbit  

1. **Continuous action ‚â† chaos** when you hand the wheel to an actor network.  
2. **Noise still matters**‚ÄîGaussian simplicity worked fine.  
3. **Hyper-parameters are rocket fuel**; a tiny œÑ tweak can nosedive or sky-rocket performance.  
4. **Baseline DDPG is good‚Äîbut TD3, SAC, or PPO could push beyond heuristic pilots.**   

---

## Final transmission  

> *‚ÄúIt‚Äôs not just about landing softly; it‚Äôs about landing softly **every** time.‚Äù*  

DDPG‚Äîwith a dash of Optuna pixie dust‚Äîdoes exactly that. Next mission: slap entropy bonuses on a Soft Actor-Critic and see if we can beat the human PID wiz. But for tonight, enjoy the view: our neural network just left the lander upright, engines off, and flags flying. üåî‚ú®  

---

**Note: If the embedded PDF is not displayed properly or if you are viewing this on a mobile device, <a href="https://kodendaal.github.io/assets/CS7642_RL___Project_2_final.pdf" target="_blank">please click here</a> to access the PDF directly.**

<div id="adobe-dc-view" style="width: 100%;"></div>
<script src="https://acrobatservices.adobe.com/view-sdk/viewer.js"></script>
<script type="text/javascript">
	document.addEventListener("adobe_dc_view_sdk.ready", function(){ 
		var adobeDCView = new AdobeDC.View({clientId: "a44769aae36f49c698c67b80c801a81e", divId: "adobe-dc-view"});
		adobeDCView.previewFile({
			content:{location: {url: "https://kodendaal.github.io/assets/CS7642_RL___Project_2_final.pdf"}},
			metaData:{fileName: "CS7642_RL___Project_2_final"}
		}, {embedMode: "IN_LINE"});
	});
</script>
